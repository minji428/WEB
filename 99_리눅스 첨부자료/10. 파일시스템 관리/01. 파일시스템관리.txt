### 파일 시스템 관리 ###



	* 파일 시스템 ( File System ) 
	
		- 파일이나 자료를 쉽게 발견 및 접근 할 수 있도록 보관 또는 조직하는 체계
		- 파일 저장이나 검색을 용이하도록 유지/관리하며
		  빠르게 읽고 사용할 수 있도록 만들어 놓은 규칙
		
		Ex) 
		도서관 		 - 하드디스크
		책	  		 - 파일
		도서검색대	 	 - 파일 시스템
		
		
		- 리눅스에서 자주 사용하는 파일 시스템
		
			ext3 , ext4 , xfs , iso9660 , nfs
		
			@ext4 ( EXTended File System 4 )
			
			- 1EB의 최대 파일 시스템 크기와 , 16TB크기의 파일을 지원
			- 서브디렉터리를 64000개 지원 , 파일은 약 40억개 지원
		
	
	* 파티션 ( Partition ) 
	
		- 운영체제가 저장 공간을 인식하는 단위
		- 하나의 물리적인 디스크를 논리적인 디스크로 나누는 것
		
		
		/dev/sda	-> 첫번째 디스크
		/dev/sdb	-> 두번째 디스크
		/dev/sdc	-> 세번째 디스크
		
		
			+@SATA , SCSI 타입의 디스크를 사용 /dev/sd?
				 IDE      타입의 디스크를 사용 /dev/hd?
		
		
		/dev/sda1	-> 첫번째 디스크 1번 파티션
		/dev/sda2	-> 첫번째 디스크 2번 파티션
		/dev/sdb1	-> 두번째 디스크 1번 파티션
		/dev/sdd4	-> 네번째 디스크 4번 파티션
		
		디스크를 추가하고 파티션과 파일 시스템을 만들어서
		( 용량 증설 , 디스크 추가 ) RAID , LVM , QUOTA 등을 이용해
		안전하고 효율적으로 디스크를 사용할 수 있다.
		
		
		
		
		! 실습
		
		
		1. 디스크 추가
		
			Vmware Workstation -> Server-A -> Settings 
			-> Add -> Hard Disk -> (설치때와 같은 옵션으로 진행)
			-> 용량 설정 -> #reboot
			
				1GB Disk 1개 추가
				
			
			+@ 가상환경에서는 추가한 디스크를 인식시키기 위해서 
			   종료후사용 및 재부팅을 하지만
			   
			   실제 서버에는 HotPlug , udev와 같은 기능이 있어서
			   추가 즉시 인식하여 사용가능하다.
		
		
		2. fdisk를 이용하여 추가된 디스크를 확인
		
			#fdisk -l		>> 모든 디스크의 정보를 출력
			
			#ls -l /dev/sd*	>> 추가된 디스크 파일 확인
		
		
		3. 리눅스 파티셔닝
		
			[ 사용법 ]
			
			#fdisk [장치파일명]
			
			Ex)
			#fdisk /dev/sdb
			#fdisk /dev/sdc
			
			( 가상 설정 )
			
			주파티션1 -> 200M 할당
			주파티션2 -> 300M 할당
			주파티션3 -> 나머지 전부 할당
			
			
			
			[root@Server ~]#fdisk /dev/sdb			<--
			Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel
			Building a new DOS disklabel with disk identifier 0x3ce2eab9.
			Changes will remain in memory only, until you decide to write them.
			After that, of course, the previous content won't be recoverable.

			Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)

			WARNING: DOS-compatible mode is deprecated. It's strongly recommended to
					 switch off the mode (command 'c') and change display units to
					 sectors (command 'u').

			Command (m for help): n							<--
			Command action
			   e   extended
			   p   primary partition (1-4)
			p												<--
			Partition number (1-4): 1						<--
			First cylinder (1-130, default 1): 				<-- "enter"
			Using default value 1
			Last cylinder, +cylinders or +size{K,M,G} (1-130, default 130): +200M    <--

			Command (m for help): p							<--

			Disk /dev/sdb: 1073 MB, 1073741824 bytes
			255 heads, 63 sectors/track, 130 cylinders
			Units = cylinders of 16065 * 512 = 8225280 bytes
			Sector size (logical/physical): 512 bytes / 512 bytes
			I/O size (minimum/optimal): 512 bytes / 512 bytes
			Disk identifier: 0x3ce2eab9

			   Device Boot      Start         End      Blocks   Id  System
			/dev/sdb1               1          26      208813+  83  Linux

			Command (m for help): n							<--
			Command action
			   e   extended
			   p   primary partition (1-4)
			p												<--
			Partition number (1-4): 2						<--
			First cylinder (27-130, default 27): 			<-- "enter"
			Using default value 27
			Last cylinder, +cylinders or +size{K,M,G} (27-130, default 130): +300M  	<--

			Command (m for help): p  						<--

			Disk /dev/sdb: 1073 MB, 1073741824 bytes
			255 heads, 63 sectors/track, 130 cylinders
			Units = cylinders of 16065 * 512 = 8225280 bytes
			Sector size (logical/physical): 512 bytes / 512 bytes
			I/O size (minimum/optimal): 512 bytes / 512 bytes
			Disk identifier: 0x3ce2eab9

			   Device Boot      Start         End      Blocks   Id  System
			/dev/sdb1               1          26      208813+  83  Linux
			/dev/sdb2              27          65      313267+  83  Linux

			Command (m for help): n							<--
			Command action
			   e   extended
			   p   primary partition (1-4)
			p												<--
			Partition number (1-4): 3						<--
			First cylinder (66-130, default 66): 			<--"enter"
			Using default value 66
			Last cylinder, +cylinders or +size{K,M,G} (66-130, default 130):	<--	"enter"
			Using default value 130

			Command (m for help): p							<--

			Disk /dev/sdb: 1073 MB, 1073741824 bytes
			255 heads, 63 sectors/track, 130 cylinders
			Units = cylinders of 16065 * 512 = 8225280 bytes
			Sector size (logical/physical): 512 bytes / 512 bytes
			I/O size (minimum/optimal): 512 bytes / 512 bytes
			Disk identifier: 0x3ce2eab9

			   Device Boot      Start         End      Blocks   Id  System
			/dev/sdb1               1          26      208813+  83  Linux
			/dev/sdb2              27          65      313267+  83  Linux
			/dev/sdb3              66         130      522112+  83  Linux

			Command (m for help): w						<--
			The partition table has been altered!

			Calling ioctl() to re-read partition table.
			Syncing disks.
			
			+@ 확장 파티션 (논리 파티션)
			
						
				[주1][확2]            [논리5]
				[주1][주2][주3][확장4][논리5]...[논리12]
				
				- 확장 파티션 공간에서 다시 논리 파티션으로 설정
				- 확장 파티션은 데이터가 저장되는 공간이 아니다.
				- 논리 파티션의 생성 범위를 지정하는 틀
				- 논리 파티션은 5번 부터 항상 시작한다.
				- [주][주][주][확장] -> 5번 부터 시작
				  [주][	   확장    ] -> 5번 부터 시작
				- 5번이상의 파티션번호를 확인 한다면
                  해당 디스크에 확장 파티션이 있다는 의미이다.

		4. 파일 시스템 생성
		
			mkfs (  MaKe File System )
			
			[ 사용법 ]
			
			#mkfs -t [타입] [장치 파일명]
			
			Ex)
			#mkfs -t ext4 /dev/sdf1		>>5번째 디스크의 첫번째 파티션에
										>>ext4형식으로 파일시스템을 생성
				
			#mkfs -t ext4 /dev/sdb1
			#mkfs -t ext4 /dev/sdb2
			#mkfs -t ext4 /dev/sdb3
			
			
			파일시스템 생성 확인
			
			[ 사용법 ]
			
			#file -s [장치파일명]
			
			#file -s /dev/sdb1
			#file -s /dev/sdb2
			#file -s /dev/sdb3
		
		
		5. 마운트 포인트 생성과 마운트
		
			- 마운트란 특정 디렉터리에 저장장치를 연결하는 것
			- 마운트포인트는 장치와 연결되는 디렉터리를 말한다.
			- 리눅스는 최상위 / 디렉터리부터 트리 구조이지만
			  윈도우는 각 장치(파티션)별 트리구조 형태를 이룬다.
		
			[ 사용법 ]
			
			#mount [장치파일명] [마운트포인트]
			
			
			#mkdir /App-{1..3}
			#mount /dev/sdb1 /App-1
			#mount /dev/sdb2 /App-2
			#mount /dev/sdb3 /App-3
		
			+@) 언마운트 : 디렉토리와 장치의 연결을 해제 한다.
			#umount [마운트포인트]		Ex) umount /App-1
			
			-  언마운트가 안되는 경우 
			   1) 해당 장치의 파일이 현재 사용 중이거나 
			   2) 현재 경로가 마운트된  경로에 있을 경우.
			
			
		6. 디스크 사용량 확인 및 디스크 사용
		
			df ( Disk Free ) - 마운트되어있는 파티션별 용량 확인
			
			[ 사용법 ]
			
			#df 
			#df -h			>>사람이 읽기 쉬운 용량단위를 사용
			
			Filesystem     1K-blocks    Used Available Use% Mounted on
			/dev/sda3       19792144 5054356  13725720  27% /
			tmpfs             502056       0    502056   0% /dev/shm
			/dev/sda1         194241   27096    156905  15% /boot
			/dev/sdb1         198123    1806    185877   1% /App-1
			/dev/sdb2         295141    2062    277416   1% /App-2
			/dev/sdb3         497444    2318    469021   1% /App-3

			
			[root@Server ~]#df -h
			Filesystem      Size  Used Avail Use% Mounted on
			/dev/sda3        19G  4.9G   14G  27% /
			tmpfs           491M     0  491M   0% /dev/shm
			/dev/sda1       190M   27M  154M  15% /boot
			/dev/sdb1       194M  1.8M  182M   1% /App-1
			/dev/sdb2       289M  2.1M  271M   1% /App-2
			/dev/sdb3       486M  2.3M  459M   1% /App-3

		
		7. UUID 확인

			UUID ( Universial Unique Identifier )
			
			- 범용 고유 식별자
			- 특정 장치나 설정에 부여하는 고유한 값
			- 디스크를 추가하거나 , 제거 작업시에 장치명이 변경되어서
			  마운트가 되지않는 현상을 사전에 방지한다.
			  
			[ 사용법 ]
			
			#blkid					>> block id 
			#blkid [장치파일명]		>> 특정장치파일의 uuid만 확인
			
			
			[root@Server ~]#blkid
			/dev/sda1: UUID="b9788f07-d875-41e3-a034-afdc8c142da3" TYPE="ext4" 
			/dev/sda2: UUID="7b5123c0-552b-4838-bc90-a33228e6594a" TYPE="swap" 
			/dev/sda3: UUID="3b29cbf6-ee82-4b1a-b2b3-b83464e69a7d" TYPE="ext4" 
			/dev/sdb1: UUID="0c2020d1-c101-4141-a8aa-c9c1c1b4ed15" TYPE="ext4" 
			/dev/sdb2: UUID="d1c6f2d4-8239-4621-9a6e-26f8726d7d5e" TYPE="ext4" 
			/dev/sdb3: UUID="1ddd2e69-0561-4e58-aa9b-496f6592331d" TYPE="ext4" 

		
		8. vi /etc/fstab 
		
			- 부팅시에 해당 파일에 작성된 정보에 따라 디렉터리구조가 완성된다. 
			
			
			#vi /etc/fstab
			
			[연결대상]	[마운트포인트]	[파일시스템] [옵션] [덤프] [검사]
			
			
			1) [연결대상]
			
			- 디스크에 생성된 파티션 장치명이오거나 UUID가 온다.
			
			2) [마운트포인트]
			
			- 장치와 연결되는 디렉터리
			
			3) [파일시스템]
			
			- 파티션에 생성한 파일시스템
			
			4) [옵션]
			
			- 마운트시 사용할 옵션을 나열하는 필드
			- 보통 defaults(rw,async,suid등이 묶여있는 옵션)를 사용한다. 
			
			5) [덤프]
			
			- 해당파티션에 문제가 발생했을 때 파티션에 대한 정보가
			  저장되어있는 파일 생성 유무
			  
			  ( 각 파티션의 최상위 디렉터리 아래 lost+found 디렉터리에 덤프 파일이 생성됨 )
			  
			  0 : 생성하지 않음
			  1 : 생성
			
			6) 검사
			
			- 부팅 시 해당 파티션에 생성된 파일 시스템 검사 유무
			
			
				0 : 검사하지 않음
				1 : 검사 ( 우선순위가 높음 )
				2 : 검사 ( 우선순위가 낮음 )
				
			- 중요한파티션에는 1을 설정하고 , 그 외의 파티션에는
			  0을 사용 (검사x)하는 것이 일반적이다.
			

			#vi /etc/fstab
			-------------------
			기존내용에 추가
			
			UUID=0c2020d1-c101-4141-a8aa-c9c1c1b4ed15       /App-1  ext4    defaults        0 0 
			UUID=d1c6f2d4-8239-4621-9a6e-26f8726d7d5e       /App-2  ext4    defaults        0 0
			UUID=1ddd2e69-0561-4e58-aa9b-496f6592331d       /App-3  ext4    defaults        0 0

			:wq!
			--------------------
			
			#init 6


			! 실습
			
			1. 스냅샷을 이용하여 Server-A를 초기화 시킨 후 작업을 진행한다.				
			2. 디스크 추가 - Server-A에 Disk 2개를 추가한다. ( 1GB, 2GB )		
			3. 재부팅 후 fdisk -l 명령어를 이용하여 추가한 Disk를 확인한다.     		
																		
			4. fdisk 명령어를 이용하여 아래와 같이 각 Disk의 파티션을 구성한다. 			
				1GB Disk - 주파티션1 - 300MB                               													
						   주파티션2 - 나머지 전부
				2GB Disk - 주파티션1 - 500MB							
						   확장 파티션2 - 나머지 전부							
						   논리 파티션5 - 나머지 전부							
					   
			5. mkfs 명령어로 각 파티션에 파일 시스템을 생성한다. ( ext4 )
			6. 아래와 같이 마운트 포인트 생성과 마운트를 진행한다.
				1GB Disk 1번 파티션 -> /APP1			1GB Disk 2번 파티션 -> /APP2
				2GB Disk 1번 파티션 -> /APP3			2GB Disk 5번 파티션 -> /APP4
			7. blkid 명령어로 추가한 파티션에 UUID를 확인 한 후 부팅 시 자동으로
			   마운트 되도록 /etc/fstab에 기본 옵션을 적용하여 추가한다.
			8. 재부팅 후 마운트가 정상적으로 되어 있는지 확인한다.
	
	
	
	* RAID ( Redundant Array Of Inexpensive/Independent Disk )
	
		- 물리 디스크 여러개를 하나의 논리 디스크로 만들어서 사용하는 방법
		
		- 초창기 RAID
		
			초기에는 디스크 용량이 크지가 않아서 디스크교체가 빈번하게 일어남
			교체한 디스크를 폐기하기에는 아깝고 단독으로 사용하기에는
			용량이 부족한(Inexpensive) 저장장치를 재활용할 목적으로 RAID를 구성
			
		- 현재 RAID
		
			저장장치의 용량이 커저 , 용량증설이 목적이 아니라 데이터 보호
			디스크의 성능을 개선할 목적으로 사용한다고 해서
			단독으로 사용가능한 (Independent)저장 장치로 해석된다.
	
	
	
		! 실습환경 구성
		
		1) 가상 머신 Server-A를 디스크관리 전으로 초기화
		2) 0.25GB Disk 총 9개 추가 
		3) 재부팅
		4) ls -l /dev/sd*
		
		brw-rw----. 1 root disk 8,   0 2018-04-04 05:23 /dev/sda
		brw-rw----. 1 root disk 8,   1 2018-04-04 05:23 /dev/sda1
		brw-rw----. 1 root disk 8,   2 2018-04-04 05:23 /dev/sda2
		brw-rw----. 1 root disk 8,   3 2018-04-04 05:23 /dev/sda3
		brw-rw----. 1 root disk 8,  16 2018-04-04 05:23 /dev/sdb
		brw-rw----. 1 root disk 8,  32 2018-04-04 05:23 /dev/sdc
		brw-rw----. 1 root disk 8,  48 2018-04-04 05:23 /dev/sdd
		brw-rw----. 1 root disk 8,  64 2018-04-04 05:23 /dev/sde
		brw-rw----. 1 root disk 8,  80 2018-04-04 05:23 /dev/sdf
		brw-rw----. 1 root disk 8,  96 2018-04-04 05:23 /dev/sdg
		brw-rw----. 1 root disk 8, 112 2018-04-04 05:23 /dev/sdh
		brw-rw----. 1 root disk 8, 128 2018-04-04 05:23 /dev/sdi
		brw-rw----. 1 root disk 8, 144 2018-04-04 05:23 /dev/sdj
		
		- dev/sda를 제외한 /dev/sdb ~ /dev/sdj까지 확인
		
		---스냅샵 저장(디스크 추가후)---
	
	
	* Linear RAID 
	
	
		- 선형 RAID
		- 첫번째 디스크가 완전히 채워지면 순차적으로 다음 디스크에 데이터를 저장
		- 최소 2개 이상의 디스크를 필요로 한다.
		- 여러개의 저장장치를 하나의 큰 논리 저장 장치 (볼륨)로 만든다.
		- 디스크의 총용량과 RAID를 구성한 볼륨의 총용량이 같다.
		
		
		! 실습
		
		1) 파티셔닝 및 타입 변경
		
		#fdisk /dev/sdb
		n				>>파티션생성
		p				>>주파티션
		1				>>1번 파티션
		enter			>>디스크의 시작부터
		enter			>>마지막지점까지
		
		t				>>파티션 타입변경
		l				>>파티션 타입 확인
		fd				>>RAID용으로 타입 변경
		p
		
		Device Boot      Start         End      Blocks   Id  System
		/dev/sdb1           1         256      262128    fd  Linux raid autodetect	>> ID와 System확인
		
		w				>>종료
		
		
		
		2) mdadm 명령어를 이용하여 RAID 구성
		
		[ 사용법]
		
		#mdadm --create [볼륨명] --level=[레벨] --raid-devices=[장치수] [장치명1] ... [장치명n]
				= -C
		
		#mdadm --create /dev/md9 --level=linear --raid-device=2 /dev/sdb1 /dev/sdc1
		mdadm: Defaulting to version 1.2 metadata
		mdadm: array /dev/md9 started.
		
		
		3) 볼륨 확인
		#cat /proc/mdstat				>> md9이 있는지 확인
		#mdadm --detail /dev/md9		>> md9의 자세한 설정 확인
		
		 - RAID 정보 파일
			/proc/mdstat		>> 자동으로 생성되어 있으며, RAID 볼륨의
								   정보가 저장되어있는 파일
			/etc/mdadm.conf 	>> 사용자가 생성해야 하며, 구성한 RAID볼륨명
								   과 설정에 관한 UUID 정보가 저장되는 파일
		
		
		4) 설정 적용
		
		#mdadm --detail --scan > /etc/mdadm.conf
		
		- 해당 명령어를 사용하지 않고 재부팅을 할 경우
		  지정한 볼륨명이 변경되어 작업에 혼선이 올 수 있다.
		  
		- 볼륨명이 지정한 이름과 다를 경우 다음과 같이 조치한다.
		
		
		* 오류 상황 시, 해결 *
		
		#vi /etc/mdadm.conf
		------------------------------------------------
		ARRAY /dev/md/Server-A:9	
		ARRAY /dev/md127			>>운영체제가 임의로 저장한 번호
							
		/dev/md127			-> /dev/md[x]
		/dev/md/Server-A:9	-> /dev/md[x]
		
		:wq!
		-------------------------------------------------
		
		
		5) 파일 시스템 생성
		#mkfs -t ext4 /dev/md9
		
		
		6) 마운트 포인트 생성 및 마운트
		#mkdir /RAID-Linear
		#mount /dev/md9 /RAID-Linear
		
		
		7) 마운트 확인
		#df -h
		
		
		8) /etc/fstab
		
		
		#blkid
		md9 UUID="~~~~~"  					>>복사
		
		#vi /etc/fstab
		----------------
		
		
		UUID=붙여넣기	/Raid-Linear ext4 defaults 0 0 
		:wq!
		-----------------
		
		
		9) 마운트 테스트
		#init 6
		#df -h
	
	
	
	
	
	* RAID 0 (stripe)	>> 순차적으로 작업 : 100G + 100G = 200G
	
		- 스트라이핑 RAID
		- 데이터를 여러디스크에 나누어 쓰고 읽어들임으로써
		  데이터를 기록하는데 지연되는 시간을 줄여 
		  가장 높은 입출력 성능을 제공한다. ( 빠르다 )
		- 데이터 보호기능이 없다.
		- RAID 0 볼륨의 총 용량은 구성한 디스크의 총용량과 같다.
		
		! 실습
		
		1) 볼륨생성
		#mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdd1 /dev/sde1
		
		
		2) 볼륨확인
		
		#cat /proc/mdstat				>> md0이 있는지 확인
		#mdadm --detail /dev/md0		>> md0의 자세한 설정 확인
		
		
		3) 설정 적용
		#mdadm --detail --scan > /etc/mdadm.conf	
		
		
		4) 파일 시스템 생성
		
		#mkfs -t ext4 /dev/md0


		5) 마운트 포인트 생성 및 마운트
		
		#mkdir /RAID-0
		#mount /dev/md0 /RAID-0
		
		#df -h									>> 마운트 확인
		
		#cp /etc/passwd	/RAID-0					>> 간단한테스트용 파일 복사
		
		
		6) vi /etc/fstab
		#blkid
		
		md0:UUID= "~~"							>> 복사
		
		#vi /etc/fstab
		----------------
		
		
		UUID=붙여넣기	/RAID-0 ext4 defaults 0 0 
		:wq!
		-----------------
		
		
		7) 테스트
		#init 6
		#df -h
	
	
	
	
	* RAID 1 (mirror)	>> 번갈아 작업/백업 : 100G + 100G(백업) = 100G	기업 선호(안전성때문)
	
		- 미러링 RAID
		- 총 Disk 용량의 50%만 사용가능
		- 데이터를 여러번 기록하기 때문에 성능(시간)이 감소
		- 한 개의 디스크가 고장이 나도 볼륨의 다른 디스크에서
		  데이터를 사용할 수 있다. ( Fault - Tolerant 기능)
		  
		@Fault-Tolerant ( 결함 감내 / 장애 내성 )
		- 결함 (Fault)이나 고장 (Failure)이 발생하여도 정상적 혹은 부분적으로
		  기능을 수행할 수 있는 환경
		
		
		
		! 실습
		
		1) 볼륨생성
		
		#mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdf1 /dev/sdg1
		
		 - 부팅 장치로 사용로 사용할 수 없다는 메시지 출력되면 y 입력하여 진행
			mdadm: array /dev/md1 started.
		
		
		2) 볼륨확인
		
		#cat /proc/mdstat				>> md1이 있는지 확인
		#mdadm --detail /dev/md1		>> md1의 자세한 설정 확인 (array size = 0.25G)
		
		
		3) 설정 적용
		
		#mdadm --detail --scan > /etc/mdadm.conf
		
		
		4) 파일 시스템 생성
		
		#mkfs -t ext4 /dev/md1


		5) 마운트 포인트 생성 및 마운트
		
		#mkdir /RAID-1
		#mount /dev/md1 /RAID-1
		
		#df -h									>> 마운트 확인

		#cp /etc/passwd	/RAID-1					>> 간단한테스트용 파일 복사
		
		
		6) vi /etc/fstab
		#blkid
		
		md1:UUID= "~~"							>> 복사
		
		#vi /etc/fstab
		----------------
		
		
		UUID=붙여넣기	/RAID-1 ext4 defaults 0 0 
		:wq!
		-----------------
		
				
		8) 테스트
		#reboot
		#df -h
	
	
	
	* RAID-5
	
		- RAID-1 처럼 데이터 결함도 허용하면서 , RAID-0처럼 공간 효율성도 좋은 방식
		- 최소 3개이상의 Disk가 필요하며, 보통은 5개 이상의 Disk로 구성을 한다. ( Disk가 많아질 수록 볼륨의 용량도 커짐)
		- Disk 장애가 발생시 패리티(Parity)를 이용하여 데이터를 복구한다.		
		- 하드디스크 개수 - 1 = 볼륨의 총 용량
		  Ex4)
		  디스크 개수 4  -> 디스크 3개의 용량
		
		
		- 패리티 (Parity) : 디스크 장애 발생 시 데이터를 재구축하는데 사용 할 수 있는 계산된 값 (데이터를 저장하지 않음)
		
			RAID 5 패리티 사용 예
				
				EX) 4개의 디스크로 구성된 RAID 5
				데이터  -> 000 111 101 010 
				데이터 저장 방향 -------->
				패리티  -> ★
				
				------------------------>
				A		B 		C		D
				0		0       0       ★
				1		1       ★       1
				1		★		0		1
				★       0		1		0
				
			
				------------------------>
				A		B 		C		D
				0		0       0      (0)
				1		1      (1)      1
				1	   (0)		0		1
			   (1)      0		1		0
			
			디스크 고장
				------------------------>
				A		B 		C		D
				0		0      [0]     (0)
				1		1      [1]      1
				1	   (0)	   [0] 		1
			   (1)      0	   [1]		0
			   
			 C Disk - 0101 
			 
		
		! 실습
		
		1) 볼륨생성
		#mdadm --create /dev/md5 --level=5 --raid-devices=3 /dev/sdh1 /dev/sdi1 /dev/sdj1
		
		
		2) 볼륨확인
		
		#cat /proc/mdstat				>> md5이 있는지 확인
		#mdadm --detail /dev/md5		>> md5의 자세한 설정 확인
		
		
		3) 설정 적용
		
		#mdadm --detail --scan > /etc/mdadm.conf
		
		
		4) 파일 시스템 생성
		
		#mkfs -t ext4 /dev/md5

		5) 마운트 포인트 생성 및 마운트
		
		#mkdir /RAID-5
		#mount /dev/md5 /RAID-5
		
		#df -h									>> 마운트 확인		
		#cp /etc/passwd	/RAID-5					>> 간단한테스트용 파일 복사
		
		
		6) vi /etc/fstab
		
		#blkid
		
		md1:UUID= "~~"							>> 복사
		
		#vi /etc/fstab
		----------------
		
		
		UUID=붙여넣기	/RAID-5 ext4 defaults 0 0 
		:wq!
		-----------------
		
		7) 설정 적용
		#mdadm --detail --scan > /etc/mdadm.conf
		
	
	
	* 장애 테스트
	
		RAID-Linear 		/dev/sdb1 , /dev/sdc1
		RAID-0				/dev/sdd1 , /dev/sde1
		RAID-1				/dev/sdf1 , /dev/sdg1
		RAID-5				/dev/sdh1 , /dev/sdi1 , /dev/sdj1
		
		
		Vmware Workstation ->    2번 , 4번 , 6번 , 8번 디스크 제거
							(/dev/sdb1 , /dev/sdd1 , /dev/sdf1 , /dev/sdh1)
		

		#reboot
		#df -h				>>linear(x) , md0(x), md1(o) ,md5(o)
	
	
	

	
	* LVM ( Logical Volume Manager )
	
		- Disk 파티션을 효율적으로 관리하고 사용할 수 있는 방식
		
		
		PV ( Physical Volume ) 물리볼륨
		
			- /dev/sdb1 , /dev/sdc1과 같은 실제 하드디스크의 파티션을 의미
		
		VG ( Volume Group )  볼륨 그룹
		
			- 여러개의 PV를 그룹으로 묶어 놓은 것을 의미
			
		LV ( Logical Volume )
		
			- VG를 다시 적절한 크기의 파티션으로 나눌 때의 
			  논리파티션을 의미
		
		LE / PE ( Logical , Physical Extent ) 
		
			- LV , PV가 가진 일정한 크기의 블록을 의미(볼륨의 단위)
	
	
	
	
	* 관련 명령어
	
	1. Volume disk 제작
	
		1) 볼륨제작
	
		#pvcreate [파티션명]
		
		Ex)
		#pvcreate /dev/sdb1
		
		2) 볼륨확인
		
		#pvs					>> 전체확인
		#pvdisplay				>> 상세확인
	
	
		3) 볼륨제거
	
		#pvremove [볼륨명]		>>PV -> Disk로 환원

		Ex)
		#pvremove /dev/sdb1
		
		
	
	2. Disk pool ( Volume 그룹 관리 )
	
	
		1) Disk pool 생성
		#vgcreate [DiskPool_Name] [pv] [pv] ...
		
		Ex)
		#vgcreate VG	/dev/sdb1 /dev/sdc1 /dev/sdd1
		
		
		2) Disk pool 확인
		
		#vgs					>> 전체 확인
		#vgdisplay				>> 상세 확인
		
		Ex)
		#vgs
		#vgdisplay				>> 상세 확인 ( 전체 )
		#vgdisplay VG			>> 상세 확인 ( 특정 Disk pool )
		
		 --- Volume group ---
	  VG Name               VG					>>	VG이름
	  Cur PV                4					>> 현재 PV의개수
	  Act PV                4					>> 활성화 되어있는 PV개수
	  VG Size               3.97 GiB			>> VG 크기
	  PE Size               4.00 MiB			>> 구성하고 있는 블록의 크기 (4MB)
	  Total PE              1016				>> 총 블록의 개수 ( 1016 ) -> 4MB * 1016
	  Alloc PE / Size       1016 / 3.97 GiB		>> 할당된 블록의 수 / 용량
	  Free  PE / Size       0 / 0   			>> 여유 블록의 수 / 용량
 

	
		
		3) Disk pool 삭제
		
		#vgremove [DiskPool_Name]
		
		Ex)
		#vgremove VG
		
		
		4) Disk pool 확장
		
		#vgextend [DiskPool_Name] [newPV]
		
		Ex)
		#vgextend VG /dev/sde1
		
		
		5) Disk pool 축소
		
		#vgreduce [DiskPool_Name] [PV]
		
		Ex)
		
		#vgreduce VG /dev/sde1

	
	
	3. Logical disk 관리
	
		1) LV생성
	
		#lvcreate -n [LV명] -l [사용량] [DiskPool_Name]
		#lvcreate -n [LV명] -L [+,-,용량] [DiskPool_Name]
		
		Ex)
		#lvcreate -n LV -l 100%FREE	VG
		#lvcreate -n LV -L +1.5G VG
		
		
		2) LV확인
		
		#lvs						>> 전체 확인
		#lvdisplay					>> 상세 확인
		
		Ex)
		#lvs
		#lvdisplay
		#lvdisplay /dev/VG/LV
		
		LV Name                LV			>> LV명
		VG Name                VG			>> VG명
		LV Size                3.97 GiB		>> LV의 크기
		Current LE             1016			>> 블록의 개수
 
			
		3) LV삭제
	
		#lvremove [/dev/VG명/LV명]
		
		Ex)
		#lvremove /dev/VG/LV
		
		
	
	
	
	! 실습
	
	1) 스냅샷을 이용하여 디스크 추가 직후로 이동
	
	2) 디스크 9개 중, 위의 4개로 작업
	
	/dev/sdb   1G
	/dev/sdc   1G
	/dev/sdd   1G
	/dev/sde   1G

	3)fdisk를 이용하여 파티션 생성
	
	#fdisk /dev/sdb
	
	n
	p
	1
	enter
	enter
	t							>> 타입 변경
	8e							>> Linux LVM으로 설정
	w							>> 저장후 종료
	
	
	@/dev/sdc , /dev/sdd , /dev/sde도 똑같이 진행
	
	fdisk -l 로 확인
	
	
	4) PV 생성
	
	#pvcreate /dev/sdb1			
	#pvcreate /dev/sdc1 /dev/sdd1 /dev/sde1
	
	
	#pvs						>> PV생성 확인	
	#pvdisplay					>> PV생성 확인
		
		
	5) VG 생성
	
	#vgcreate VG  /dev/sdb1 /dev/sdc1 /dev/sdd1
	
	#vgs						>> VG 생성 확인
	#vgdisplay					>> VG 생성 확인
	
	#pvs						>> VG에 포함되었는지 확인(attr 변화)
	
	
	6) LV 생성
	
	#lvcreate -n LV -l 100%FREE	VG
	= lvcreate --name LV -l 100%FREE	VG
	
	
	7) 파일시스템 생성
	
	#mkfs -t ext4 /dev/VG/LV	
	
	
	8) 마운트 포인트 생성 및 마운트
	
	#mkdir /LVM
	#mount /dev/VG/LV /LVM
	#df -h						>> 용량 확인
	
	
	9) /etc/fstab
	
	#blkid
	/dev/mapper/VG-LV=[UUID=""] 복사
	
	--------------------------------
	
	맨 하단에
	UUID=[UUID복사] /LVM ext4 defaults 0 0
	
	:wq!
	----------------------------------
	
	
	10) LV용량 증설
	
	#cp -r /usr/*	/LVM					>> 간단한 테스트
	
	#vgextend VG /dev/sde1					>> VG에 /dev/sde1추가
	
	#lvextend -l +100%FREE /dev/VG/LV		>> LV에 추가
	
	#df -h									>> 변경 전(용량 그대로)
	
	#resize2fs /dev/VG/LV					>> 파일시스템의 크기를 재조정
	
	#df -h									>> 재조정된 크기 확인(용량 증가)
	
	
	
	
	

	+@추가 실습
	1) 디스크 추가 및 파티셔닝
	/dev/sdb1	0.25G
	/dev/sdc1	0.25G
	/dev/sdd1	0.25G
	/dev/sde1	0.25G
	/dev/sdf1	0.25G
	/dev/sdg1	0.25G
	/dev/sdh1	0.25G
	/dev/sdi1	0.25G
	/dev/sdj1	0.25G
	
	2) LV생성 및 마운트
	[ /dev/sdb1 , /dev/sdc1 , /dev/sdd1 ] -> VG1 -> LV1 -> /LVM1 마운트
	[ /dev/sde1 , /dev/sdf1 , /dev/sdg1 ] -> VG2 -> LV2 -> /LVM2 마운트
	
	3) /etc/fstab에 등록 , /usr/*파일을 마운트 포지션으로 복사
	
	4) /dev/sdh1를 LV1에 증설
	   /dev/sdi1 , /dev/sdj1를 LV2에 증설
	
